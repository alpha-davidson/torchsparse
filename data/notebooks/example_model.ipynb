{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127f5855-f00c-4409-8a93-cbbeb2627ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "from typing import Any, Dict\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchsparse\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse import nn as spnn\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd4e88d-ecc5-47b1-99a3-1eaa8d2569ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4])\n",
      "torch.Size([10000, 4])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.uniform(-100, 100, size=(10000, 4))\n",
    "labels = np.random.choice(10, size=10000)\n",
    "\n",
    "coords, feats = inputs[:, :3], inputs\n",
    "coords -= np.min(coords, axis=0, keepdims=True)\n",
    "coords, indices = sparse_quantize(coords,\n",
    "                                  0.2,\n",
    "                                  return_index=True)\n",
    "\n",
    "coords = np.column_stack((batches, coords))\n",
    "\n",
    "coords = torch.tensor(coords, dtype=torch.int)\n",
    "feats = torch.tensor(feats[indices], dtype=torch.float)\n",
    "labels = torch.tensor(labels[indices], dtype=torch.long)\n",
    "\n",
    "print(coords.shape)\n",
    "print(feats.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "input = SparseTensor(coords=coords, feats=feats)\n",
    "label = SparseTensor(coords=coords, feats=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd39d6ed-189c-4586-bba0-539131321f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = input, label\n",
    "dataflow = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=sparse_collate_fn,\n",
    ")\n",
    "\n",
    "# Instead of args.device\n",
    "device = 'cuda'  # or 'cpu' depending on your preference\n",
    "\n",
    "# Instead of args.amp_enabled\n",
    "amp_enabled = True  # Set to True or False based on your preference\n",
    "\n",
    "model = nn.Sequential(\n",
    "    spnn.Conv3d(4, 32, 3),\n",
    "    spnn.BatchNorm(32),\n",
    "    spnn.ReLU(True),\n",
    "    spnn.Conv3d(32, 64, 2, stride=2),\n",
    "    spnn.BatchNorm(64),\n",
    "    spnn.ReLU(True),\n",
    "    spnn.Conv3d(64, 64, 2, stride=2, transposed=True),\n",
    "    spnn.BatchNorm(64),\n",
    "    spnn.ReLU(True),\n",
    "    spnn.Conv3d(64, 32, 3),\n",
    "    spnn.BatchNorm(32),\n",
    "    spnn.ReLU(True),\n",
    "    spnn.Conv3d(32, 10, 1),\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25eb6256-4846-4a97-912f-672e917d15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[step 1] loss = 2.359276533126831\n",
      "0\n",
      "[inference step 1] loss = 2.3025295734405518\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scaler = amp.GradScaler(enabled=amp_enabled)\n",
    "\n",
    "for k, feed_dict in enumerate(dataflow):\n",
    "    inputs = feed_dict[0].to(device=device)\n",
    "    labels = feed_dict[1].to(device=device)\n",
    "    print(k)\n",
    "    with amp.autocast(enabled=amp_enabled):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.feats, labels.feats)\n",
    "    \n",
    "    print(f'[step {k + 1}] loss = {loss.item()}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "# enable torchsparse 2.0 inference\n",
    "model.eval()\n",
    "# enable fused and locality-aware memory access optimization\n",
    "torchsparse.backends.benchmark = True  # type: ignore\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k, feed_dict in enumerate(dataflow):\n",
    "        inputs = feed_dict[0].to(device=device).half()\n",
    "        labels = feed_dict[1].to(device=device)\n",
    "        print(k)\n",
    "        with amp.autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.feats, labels.feats)\n",
    "\n",
    "        print(f'[inference step {k + 1}] loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58fd2c-857f-4145-ac55-451c881221f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset:\n",
    "\n",
    "    def __init__(self, input_size: int, voxel_size: float) -> None:\n",
    "        self.input_size = input_size # load in the data\n",
    "        self.voxel_size = voxel_size\n",
    "\n",
    "    def __getitem__(self, _: int) -> Dict[str, Any]:\n",
    "        inputs = np.random.uniform(-100, 100, size=(self.input_size, 4)) # replace with data\n",
    "        labels = np.random.choice(10, size=self.input_size)\n",
    "\n",
    "        coords, feats = inputs[:, :3], inputs\n",
    "        coords -= np.min(coords, axis=0, keepdims=True)\n",
    "        coords, indices = sparse_quantize(coords,\n",
    "                                          self.voxel_size,\n",
    "                                          return_index=True)\n",
    "        \n",
    "        coords = torch.tensor(coords, dtype=torch.int)\n",
    "        feats = torch.tensor(feats[indices], dtype=torch.float)\n",
    "        labels = torch.tensor(labels[indices], dtype=torch.long)\n",
    "\n",
    "        print(coords.shape)\n",
    "        print(feats.shape)\n",
    "        print(labels.shape)\n",
    "        \n",
    "        input = SparseTensor(coords=coords, feats=feats)\n",
    "        label = SparseTensor(coords=coords, feats=labels)\n",
    "\n",
    "        \n",
    "        return {'input': input, 'label': label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f849606-1e1d-4d9a-96df-f4f8fa791d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 64, 64])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Create a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single data sample and its corresponding label\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Step 2: Create an instance of your custom dataset\n",
    "data = torch.randn(100, 3, 64, 64)  # Random data for demonstration\n",
    "labels = torch.randint(0, 10, (100,))  # Random labels for demonstration\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfe4fc53-f401-46e3-9a54-cce82ea62cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Data shape: torch.Size([16, 3, 64, 64])\n",
      "Labels: tensor([7, 4, 4, 0, 4, 8, 6, 6, 3, 7, 2, 0, 5, 1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "custom_dataset = CustomDataset(data, labels)\n",
    "\n",
    "# Step 3: Create a DataLoader\n",
    "batch_size = 16\n",
    "shuffle = True  # Shuffle the data during each epoch\n",
    "num_workers = 4  # Number of CPU processes to use for data loading (for parallelism)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    custom_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Step 4: Iterate over the DataLoader\n",
    "for batch_idx, (batch_data, batch_labels) in enumerate(data_loader):\n",
    "    # 'batch_data' is a batch of input data\n",
    "    # 'batch_labels' is a batch of corresponding labels\n",
    "    \n",
    "    # In a real training loop, you would perform your training steps here,\n",
    "    # e.g., forward and backward passes through a neural network.\n",
    "    \n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Data shape:\", batch_data.shape)\n",
    "    print(\"Labels:\", batch_labels)\n",
    "    \n",
    "    # For simplicity, let's break the loop after one iteration\n",
    "    if batch_idx == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e21a95-c7a8-4da2-a52e-daad4584c598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
