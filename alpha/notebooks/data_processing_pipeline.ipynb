{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4461ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605970b",
   "metadata": {},
   "source": [
    "# User Input Desired Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f3ed9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file directory to the .h5 data file to convert\n",
    "file = h5py.File('../mg22simulated/output_digi_HDF_Mg22_Ne20pp_8MeV.h5', 'r')\n",
    "\n",
    "original_keys = list(file.keys())\n",
    "original_length = len(original_keys)\n",
    "# print(original_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96eb1908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6642\n"
     ]
    }
   ],
   "source": [
    "#making an array of the lengths of events\n",
    "event_lens = np.zeros(original_length, int)\n",
    "for i in range(original_length):\n",
    "    event = original_keys[i]\n",
    "    event_lens[i] = len(file[event])\n",
    "\n",
    "discards = 0\n",
    "for i in event_lens:\n",
    "    if i < 128:\n",
    "        discards += 1\n",
    "\n",
    "print(discards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "210c2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT CLASSIFICATION\n",
      "BINARY\n",
      "[0, 1, 2, 4, 5, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1476 #enter the size to which events will be up/downsampled\n",
    "\n",
    "# designed for data in the following format:\n",
    "# x[0] ,y[1] ,z[2] ,time[3], Amplitude[4], trackID (particle ID)[5], pointID[6]\n",
    "# energy[7] ,energy loss[8] ,angle[9], Mass[10], Atomic number[11], Event_id index[12], number of tracks[13]\n",
    "# ^ this is thus designed for simulated data, as much of this will not be known for experimental data.\n",
    "\n",
    "# Enter in the CLASSIFICATION and PROJECTIONS to evaluate in all caps as projection:\n",
    "# B is BINARY, T is tertiary, and FOUR is four track classiification\n",
    "# Set TRACK_CLASS to true if performing track classification (i.e. using scannet) and to false if performing event\n",
    "# classification (i.e. using modelnet)\n",
    "TRACK_CLASS = False\n",
    "class_type = 'BINARY' # type of track classification\n",
    "PROJECTION = 'XYZQ' # the three dimensions which will be input into the model\n",
    "ISOTOPE = 'Mg22'\n",
    "PROJ_TO_COLS = [0,1,2,4,5,12,13]\n",
    "\n",
    "PROJ_TO_COLS = {'XYZQ' : [0,1,2,4,5,12,13], 'XYZ': [0,1,2,5,12,13], 'XYQ': [0,1,4,5,12,13], 'XZQ': [0,2,4,5,12,13], 'YZQ': [1,2,4,5,12,13]}\n",
    "\n",
    "user_input = PROJ_TO_COLS[PROJECTION]\n",
    "print('TRACK CLASSIFICATION' if TRACK_CLASS else 'EVENT CLASSIFICATION')\n",
    "print(class_type)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259afc1f",
   "metadata": {},
   "source": [
    "# Convert Raw H5 File into npArray with Corresponding key index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96dd7c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#making a numpy array of the data. three dimension are [event number, point within event, data value at point]\n",
    "#length of each event is based on the longest event in dataset, so non-maximal events are padded with zeros at the end\n",
    "#12th index of each data point now corresponds to the index of the event in the h5 file's original_keys\n",
    "# each point thus contains:\n",
    "# x,y,z, time, Amplitude, trackID (particle ID), pointID, energy, energy loss, angle, Mass, Atomic number, Event_id index\n",
    "file_name =  ISOTOPE + '_w_key_index'\n",
    "# **only doing this if the file doens't exist already, as the conversion takes a while**\n",
    "if not os.path.exists( 'data/' + file_name + '.npy'):\n",
    "    event_data = np.zeros((original_length, np.max(event_lens), 13), float) \n",
    "    for n in tqdm.tqdm(range(len(original_keys))):\n",
    "        name = original_keys[n]\n",
    "        event = file[name]\n",
    "        ev_len = len(event)\n",
    "        #converting event into an array\n",
    "        for i,e in enumerate(event):\n",
    "            instant = np.array(list(e))\n",
    "            event_data[n][i][:12] = np.array(instant)\n",
    "            event_data[n][i][-1] = float(n) #insert index value to find corresponding event ID\n",
    "    np.save( 'data/' + file_name, event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed18797",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assertion Statements to Check the Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b596fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load( 'data/' + ISOTOPE + '_w_key_index' + '.npy')\n",
    "assert data.shape == (original_length, np.max(event_lens), 13), 'Array has incorrect shape'\n",
    "assert len(np.unique(data[:,:,12])) == original_length, 'Array has incorrect Event_ids'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3440c",
   "metadata": {},
   "source": [
    "# Random sample From New Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2d61439-aff4-4ea8-b585-e1efbf8de3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.45410442e+00 -7.05056143e+00  8.59200000e+02  3.06000000e+02\n",
      "  2.41984875e+02  0.00000000e+00  3.00000000e+00  0.00000000e+00\n",
      "  1.54016976e-04  0.00000000e+00  2.20000000e+01  1.20000000e+01\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b6bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (42,13) into shape (1476,13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m shortest_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(particle_ids \u001b[38;5;241m==\u001b[39m shortest)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_data)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m data[i][:ev_len,:]\n\u001b[1;32m     17\u001b[0m unique_point_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(data[i,:ev_len,\u001b[38;5;241m5\u001b[39m])    \u001b[38;5;66;03m#array of unique particle IDs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m new_data[i][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m unique_point_ids\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m#number of unique particles, scaled to start at 0\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (42,13) into shape (1476,13)"
     ]
    }
   ],
   "source": [
    "#adding 13th column to correspond to the number of tracks in event, zero-indexed: 0 = beam, 1= two track, 2 = 3 track...\n",
    "data_array = ISOTOPE + '_w_key_index.npy' #insert desired array to sample from \n",
    "new_array_name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + data_array)\n",
    "new_data = np.zeros((original_length, sample_size, 14), float)\n",
    "count = 0\n",
    "for i in range(original_length):\n",
    "    ev_len = event_lens[i]    #length of event-- i.e. number of points\n",
    "    particle_ids = data[i][:ev_len,5]\n",
    "    label, distr = np.unique(particle_ids, return_counts=True)\n",
    "    shortest = label[np.argmin(distr)]\n",
    "    shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "    print(new_data)\n",
    "    \n",
    "    new_data[i][:,:-1] = data[i][:ev_len,:]\n",
    "    \n",
    "    unique_point_ids = np.unique(data[i,:ev_len,5])    #array of unique particle IDs\n",
    "    new_data[i][0][-1] = unique_point_ids.size - 1    #number of unique particles, scaled to start at 0\n",
    "# np.save('data/' + new_array_name, new_data) \n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i in range(len(new_data)):\n",
    "\n",
    "    if new_data[i][:, :-1].any() == np.zeros((128, 13)).all():\n",
    "        idxs.append(i)\n",
    "\n",
    "idxs = np.asarray(idxs)\n",
    "\n",
    "# print(len(idxs))\n",
    "# print(count)\n",
    "assert len(idxs) == count\n",
    "\n",
    "cut_new_data = np.ndarray((len(new_data)-len(idxs), 128, 14))\n",
    "\n",
    "inc = 0\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "\n",
    "    if i not in idxs:\n",
    "        cut_new_data[inc] = new_data[i]\n",
    "        inc += 1\n",
    "\n",
    "\n",
    "np.save('data/' + new_array_name, cut_new_data) \n",
    "print(cut_new_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6e60",
   "metadata": {},
   "source": [
    "### Assertion Statements to Check the Data After Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfeed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load( 'data/' + ISOTOPE + '_size' + str(sample_size) + '_sampled.npy')\n",
    "assert data.shape == (original_length-count, sample_size, 14), 'Array has incorrect shape -- '+str(data.shape)\n",
    "assert len(np.unique(data[:,:,13])) == len(np.unique(data[:,:,5]))-1, 'Array has incorrect number of tracks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c76ba94",
   "metadata": {},
   "source": [
    "### Check Distribution of labels after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c30425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheking how the distribution of labels changes from sampling\n",
    "name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + name + '.npy')\n",
    "real_tracks = np.zeros(original_length,int) \n",
    "sampled_tracks = np.zeros(original_length,int)\n",
    "\n",
    "for i in range(original_length-count):\n",
    "    ev_nt = data[i]\n",
    "    real_tracks[i] = ev_nt[0,-1]\n",
    "    unique_point_ids = np.unique(ev_nt[:,5])    #array of unqiue particles IDs\n",
    "    sampled_tracks[i] = unique_point_ids.size - 1\n",
    "    \n",
    "og_label, og_distr = np.unique(real_tracks, return_counts=True)\n",
    "new_label, new_distr = np.unique(sampled_tracks, return_counts=True)\n",
    "print(og_label, new_label)\n",
    "print(og_distr)\n",
    "print(new_distr)\n",
    "# number of events that have lost a track from sampling. not a big deal if nonzero\n",
    "print('Events changed = ' + str(np.sum(np.abs(new_distr - og_distr))//2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6bd26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Filtered npArrays\n",
    "For track classification, focusing on 4-track events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TRACK_CLASS, 'only do this for track classification'\n",
    "file_name = ISOTOPE + '_size' + str(sample_size) + '_sampled' #insert desired file name to open\n",
    "raw_data = np.load('data/' + file_name + '.npy')\n",
    "new_file_name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "count = 0\n",
    "new_data = np.zeros((original_length, sample_size, 14), float)\n",
    "\n",
    "for i in range(original_length):\n",
    "    new_event = raw_data[i]\n",
    "    unique_point_ids = np.unique(new_event[:,5])    \n",
    "    current_tracks = unique_point_ids.size - 1 \n",
    "    og_tracks = new_event[0,-1]\n",
    "    \n",
    "    #omitting non-4-track events, mislabeled events, and that one event with a particle ID 4\n",
    "    if og_tracks != 3 or og_tracks != current_tracks or 5 in unique_point_ids:\n",
    "        continue\n",
    "    else:\n",
    "        new_event[:,5] -= 1 # lowering particle id to start from 0\n",
    "        new_data[count,:,:] = new_event\n",
    "        count += 1\n",
    "    \n",
    "print(count)\n",
    "saving = new_data[:count, :,:]\n",
    "np.save('data/' + new_file_name, saving) #creating new np array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0357fa",
   "metadata": {},
   "source": [
    "### Assertion Statements to Check the Data After Filtering Tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK_CLASS:\n",
    "    data = np.load( 'data/' + ISOTOPE + '_4-track_size' + str(sample_size)+ '.npy')\n",
    "    print(list(np.unique(data[:,:,5])))\n",
    "    assert data.shape == (count, sample_size, 14), 'Array has incorrect shape'\n",
    "    assert list(np.unique(data[:,:,5])) == [0.0, 1.0, 2.0, 3.0], 'Array has incorrect particle ids'\n",
    "    assert len(np.unique(data[:,:,5])) == 4, 'Array has incorrect number of particle ids'\n",
    "    assert list(np.unique(saving[:,0,13])) == [3.0], 'Array has incorrect type of events'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43669d",
   "metadata": {},
   "source": [
    "### Check Distribution of Event Types After Filtering Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "else:\n",
    "    name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + name + '.npy')\n",
    "real_tracks = np.zeros(len(data),int)\n",
    "sampled_tracks = np.zeros(len(data),int)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    ev_nt = data[i]\n",
    "    real_tracks[i] = ev_nt[0,-1] # original event label\n",
    "    unique_point_ids = np.unique(ev_nt[:,5])    # array of unqiue particles IDs-- i.e current event label after sampling\n",
    "    sampled_tracks[i] = unique_point_ids.size - 1\n",
    "    \n",
    "label, og_distr = np.unique(real_tracks, return_counts=True)\n",
    "label, new_distr = np.unique(sampled_tracks, return_counts=True)\n",
    "\n",
    "print(list(np.unique(data[:,:,5]))) # Particles present in data\n",
    "print(list(np.unique(data[:,:,13]))) # types of events present in data\n",
    "print(og_distr) \n",
    "print(new_distr) \n",
    "unique_point_id = np.unique(data[:,:,5])\n",
    "print((unique_point_id)) # uncomment to check particle ids\n",
    "assert (np.sum(np.abs(new_distr - og_distr))) == 0, 'Event labels do not match actual event types'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfcd9d",
   "metadata": {},
   "source": [
    "# Binary, Tertiary, and Other Classification\n",
    "### *Skip this step if doing event classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to adjust point labels based on track classification type, with particle 0 being the alpha\n",
    "# No need to do this step if doing event classification\n",
    "if TRACK_CLASS:\n",
    "    if class_type == 'BINARY':\n",
    "        def alpha(p):\n",
    "            if int(p) == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "    elif class_type == 'TERTIARY':\n",
    "        def alpha(p):\n",
    "                if int(p) == 0:\n",
    "                    return 0\n",
    "                elif int(p) == 1:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab37aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to do this step if doing event classification\n",
    "#adjusting particle IDs\n",
    "if TRACK_CLASS:\n",
    "    if class_type == 'BINARY' or class_type == 'TERTIARY':\n",
    "        name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "        data = np.load('data/' + name + '.npy')\n",
    "        new_classification = np.zeros((len(data), sample_size, 14), float)\n",
    "        for i in tqdm.tqdm(range(len(data))):\n",
    "            event = data[i]\n",
    "            new_event = event[:,:]\n",
    "            new_event[:,5] = list(map(alpha, event[:,5]))\n",
    "            new_classification[i,:,:] = new_event[:,:]\n",
    "    np.save('data/' + name, new_classification)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1c8a1",
   "metadata": {},
   "source": [
    "# Split Testing Set, Training Set, and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a78d8",
   "metadata": {},
   "source": [
    "## Split Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93821700",
   "metadata": {},
   "source": [
    "Performs a 20-test 20-val 60-train split on all 4-track events, and generates an array of numbers as long as the length of the data to randomize the events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6efa7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "    all_events = np.load('data/' + name + '.npy')\n",
    "else:\n",
    "    name = ISOTOPE + '_size' + str(sample_size)\n",
    "    all_events = np.load('data/' + name + '_sampled.npy')\n",
    "\n",
    "rand_shuffle = np.random.choice(len(all_events), len(all_events), replace = False)\n",
    "\n",
    "\n",
    "# 20-20 marking for test and validation\n",
    "test_split = int(len(all_events) * .2)\n",
    "val_split = int(len(all_events) * .4)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    test_event_indices =  all_events[rand_shuffle[:test_split],:,-2:]    #only saving the indices and number of tracks of the test events\n",
    "else:\n",
    "    test_data = all_events[rand_shuffle[:test_split],:,:]\n",
    "val_data = all_events[rand_shuffle[test_split:val_split],:,:]\n",
    "train_data = all_events[rand_shuffle[val_split:],:,:]\n",
    "\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    print(test_event_indices.shape, val_data.shape, train_data.shape)\n",
    "    np.save('data/{}_4-track_testevent_indices'.format(ISOTOPE), test_event_indices)\n",
    "else:\n",
    "    print(test_data.shape, val_data.shape, train_data.shape)\n",
    "    np.save('data/' + name + 'test', test_data)\n",
    "np.save('data/' + name + 'train', train_data)\n",
    "np.save('data/' + name + 'val', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f06867",
   "metadata": {},
   "source": [
    "# Making Test Sets\n",
    "#### We make a pair of test sets to span all the data points, ensuring that each point is tested. This is useful for testing events that have been downsampled.\n",
    "*This is only necessary for semantic segmentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an array of the 4-track test event lengths\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    ev_indices = np.load('data/' + name + '_testevent_indices.npy')\n",
    "    num_events = ev_indices.shape[0]\n",
    "    test_ev_lens = np.zeros(num_events,int)\n",
    "    for i in tqdm.tqdm(range(num_events)):\n",
    "        event_ind = int(ev_indices[i,0,0])\n",
    "        test_ev_lens[i] = event_lens[event_ind]\n",
    "    np.save('data/' + name + '_testevent_lengths', test_ev_lens)\n",
    "    print(np.max(test_ev_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33320531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an array of all the 4-track test data\n",
    "if TRACK_CLASS:\n",
    "    data = np.load('data/' + ISOTOPE + '_w_key_index.npy')\n",
    "    num_events = ev_indices.shape[0]\n",
    "    test_data = np.zeros((num_events,np.max(test_ev_lens),14),float)\n",
    "    instant = 0\n",
    "    for i in tqdm.tqdm(range(num_events)):\n",
    "        event_ind = int(ev_indices[i,0,0])\n",
    "        test_data[instant,:,:-1] = data[event_ind,:np.max(test_ev_lens),:]\n",
    "        test_data[instant,0,-1] = ev_indices[i,0,1]\n",
    "        test_data[instant,:,5] -= 1   #scaling the labels so they start at 0\n",
    "        instant += 1\n",
    "    np.save('data/' + name + '_testevents', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a normal sampled test dataset\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    test_events = np.load('data/' + name + '_testevent_indices.npy')\n",
    "    num_events = test_data.shape[0]\n",
    "    test_sample = np.zeros((num_events,sample_size,14),float)\n",
    "    incl_points = np.zeros((num_events,sample_size),int)\n",
    "    for i in range(num_events):\n",
    "        ev_len = test_ev_lens[i]    #accessing the event index to find event length\n",
    "        particle_ids = test_data[i,:ev_len,5]\n",
    "        label, distr = np.unique(particle_ids, return_counts=True)\n",
    "        shortest = label[np.argmin(distr)]\n",
    "        shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "        if ev_len == sample_size:    #if array is already preferred length\n",
    "            test_sample[i,:,:] = test_data[i,:ev_len,:]\n",
    "            incl_points[i,:] = range(sample_size)\n",
    "        else:\n",
    "            instant = 0\n",
    "            for n in range(shortest_ind.size):    #the first instances sampled will be those belonging to the shortest track\n",
    "                test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                incl_points[i,instant] = shortest_ind[n]\n",
    "                instant += 1\n",
    "            need = sample_size - shortest_ind.size\n",
    "            random_points = np.random.choice(range(ev_len), need, replace= True if need > ev_len else False)  #choosing the random instances to sample\n",
    "            for r in random_points:\n",
    "                test_sample[i,instant,:] = test_data[i,r,:] \n",
    "                incl_points[i,instant] = r\n",
    "                instant += 1\n",
    "        test_sample[i,:,6] = incl_points[i,:]    #storing the indices in the original event as point IDs\n",
    "        test_sample[i,:,5] = list(map(alpha, test_sample[i,:,5]))    #making it BINARY\n",
    "    np.save('data/' + name + '_size{}test1'.format(sample_size), test_sample)\n",
    "\n",
    "    #array of the number of points not included in the first sample\n",
    "    not_incl = np.zeros(num_events, int)\n",
    "    for i in range(num_events):\n",
    "        incl = np.unique(incl_points[i])\n",
    "        not_incl[i] = event_lens[int(test_data[i,0,-2])] - incl.size\n",
    "    \n",
    "    #indices of the points not included in the first sample\n",
    "    not_incl_points = np.zeros((num_events,np.max(not_incl)),int)\n",
    "    for i in range(num_events):    #going through each event\n",
    "        count = 0\n",
    "        for p in range(event_lens[int(test_data[i,0,-2])]):    #going through each instant in the event\n",
    "            if p not in incl_points[i]:    #if that instant is not in the included points for that event\n",
    "                not_incl_points[i, count] = p\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a second test dataset that includes the points not in the first test dataset\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    test_data = np.load('data/' + name + '_testevents.npy')\n",
    "    num_events = test_data.shape[0]\n",
    "    test_sample = np.zeros((num_events,sample_size,14),float)\n",
    "    incl_points = np.zeros((num_events,sample_size),int)\n",
    "    for i in range(num_events):\n",
    "        ev_len = test_ev_lens[i]    #accessing the event index to find event length\n",
    "        particle_ids = test_data[i,:event_lens[i],5]\n",
    "        label, distr = np.unique(particle_ids, return_counts=True)\n",
    "        shortest = label[np.argmin(distr)]\n",
    "        shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "        indices_sampled = np.zeros(sample_size, int)\n",
    "        if ev_len == sample_size:    #if array is already preferred length\n",
    "            test_sample[i,:,:] = test_data[i,:ev_len,:]\n",
    "            indices_sampled = range(sample_size)\n",
    "        else:\n",
    "            instant = 0\n",
    "            # including points not in the first set\n",
    "            for n in range(not_incl[i]):\n",
    "                test_sample[i,instant,:] = test_data[i,not_incl_points[i,n],:]\n",
    "                indices_sampled[instant] = not_incl_points[i,n]\n",
    "                instant += 1\n",
    "            if shortest_ind.size + not_incl[i] > sample_size:\n",
    "                #including as many points of shortest track as possible\n",
    "                for n in range(sample_size - not_incl[i]):    \n",
    "                    test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                    indices_sampled[instant] = shortest_ind[n]\n",
    "                    instant += 1\n",
    "            else:\n",
    "                #including all of shortest track\n",
    "                for n in range(shortest_ind.size):    \n",
    "                    test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                    indices_sampled[instant] = shortest_ind[n]\n",
    "                    instant += 1\n",
    "                random_points = np.random.choice(range(ev_len), sample_size - shortest_ind.size - not_incl[i])\n",
    "                #randomly sampling to get up to sample size\n",
    "                for r in random_points:\n",
    "                    test_sample[i,instant,:] = test_data[i,r,:]\n",
    "                    indices_sampled[instant] = r\n",
    "                    instant += 1\n",
    "        test_sample[i,:,6] = indices_sampled[:]    #storing the indices in the original event as point IDs\n",
    "        test_sample[i,:,5] = list(map(alpha, test_sample[i,:,5]))   \n",
    "    np.save('data/' + name + '_size{}test2'.format(sample_size), test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd6dfa",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '{}.npy'\n",
    "    prev_data = np.load(name.format(''))\n",
    "    tr = np.load(name.format('train'))\n",
    "    va = np.load(name.format('val'))\n",
    "    te1 = np.load(name.format('test1'))\n",
    "    te2 = np.load(name.format('test2'))\n",
    "    print(len(prev_data))\n",
    "    print(tr.shape, va.shape, te1.shape, te2.shape)\n",
    "    print(len(np.unique(tr[:,:,5])))\n",
    "    print(len(np.unique(va[:,:,12])))\n",
    "    print(len(np.unique(te1[:,:,12])))\n",
    "    print(len(np.unique(te2[:,:,12])))\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + '{}.npy'\n",
    "    prev_data = np.load(name.format('_sampled'))\n",
    "    tr = np.load(name.format('train'))\n",
    "    va = np.load(name.format('val'))\n",
    "    te = np.load(name.format('test'))\n",
    "    print(len(prev_data))\n",
    "    print(tr.shape, va.shape, te.shape)\n",
    "    print(len(np.unique(tr[:,:,5])))\n",
    "    print(len(np.unique(va[:,:,12])))\n",
    "    print(len(np.unique(te[:,:,12])))\n",
    "\n",
    "# works perfectly if length of dataset is even; if odd, rounding may be off\n",
    "# assert tr.shape == (np.ceil(len(prev_data) * .6) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "# assert va.shape == (np.ceil(len(prev_data) * .2) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "# assert te.shape == (np.ceil(len(prev_data) * .2) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "print(len(prev_data))\n",
    "if TRACK_CLASS:\n",
    "    print(tr.shape, va.shape, te1.shape, te2.shape)\n",
    "else:\n",
    "    print(tr.shape, va.shape, te.shape)\n",
    "print(len(np.unique(tr[:,:,5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc7632",
   "metadata": {},
   "source": [
    "# Rescaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5160c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.unique(np.isnan(tr[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "assert len(np.unique(np.isnan(va[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert len(np.unique(np.isnan(te1[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "    assert len(np.unique(np.isnan(te2[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "else:\n",
    "    assert len(np.unique(np.isnan(te[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "assert np.any(tr[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "assert np.any(va[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "if TRACK_CLASS:\n",
    "    assert np.any(te1[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "    assert np.any(te2[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "else:\n",
    "    assert np.any(te[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "\n",
    "scaled_val_data = copy.deepcopy(va)\n",
    "scaled_train_data = copy.deepcopy(tr)\n",
    "if TRACK_CLASS:\n",
    "    scaled_test_data1 = copy.deepcopy(te1)\n",
    "    scaled_test_data2 = copy.deepcopy(te2)\n",
    "else:\n",
    "    scaled_test_data = copy.deepcopy(te)\n",
    "# checking shapes\n",
    "if TRACK_CLASS:\n",
    "    print(scaled_test_data1.shape, scaled_test_data2.shape, scaled_train_data.shape, scaled_val_data.shape)\n",
    "else:\n",
    "    print(scaled_test_data.shape, scaled_train_data.shape, scaled_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1628d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale all charges to reduce large range of values\n",
    "if TRACK_CLASS:\n",
    "    scaled_test_data1[:,:,4] = np.log10(te1[:,:,4] + 1e-10)\n",
    "    scaled_test_data2[:,:,4] = np.log10(te2[:,:,4] + 1e-10)\n",
    "else:\n",
    "    scaled_test_data[:,:,4] = np.log10(te[:,:,4] + 1e-10)\n",
    "scaled_train_data[:,:,4] = np.log10(tr[:,:,4] + 1e-10)\n",
    "scaled_val_data[:,:,4] = np.log10(va[:,:,4] + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439dcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values correspond to the x,y,z,charge indices\n",
    "values = [0,1,2,4] \n",
    "means_and_stds = []\n",
    "# standard scaling \n",
    "for n in values:\n",
    "    mean = np.mean(scaled_train_data[:,:,n])\n",
    "    std = np.std(scaled_train_data[:,:,n])\n",
    "    means_and_stds.append([mean,std])\n",
    "    scaled_train_data[:,:,n] = (scaled_train_data[:,:,n] - mean) / std\n",
    "    scaled_val_data[:,:,n] = (scaled_val_data[:,:,n] - mean) / std\n",
    "    if TRACK_CLASS:\n",
    "        scaled_test_data1[:,:,n] = (scaled_test_data1[:,:,n] - mean) / std\n",
    "        scaled_test_data2[:,:,n] = (scaled_test_data2[:,:,n] - mean) / std\n",
    "    else:\n",
    "        scaled_test_data[:,:,n] = (scaled_test_data[:,:,n] - mean) / std\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + 'scaled_{}'\n",
    "    np.save(name.format('test_data1'), scaled_test_data1)\n",
    "    np.save(name.format('test_data2'), scaled_test_data2)\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + 'scaled_{}'\n",
    "    np.save(name.format('test_data'), scaled_test_data)\n",
    "    \n",
    "np.save(name.format('mean_and_std_data'), means_and_stds)    \n",
    "np.save(name.format('train_data'), scaled_train_data)\n",
    "np.save(name.format('val_data'), scaled_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.sum(np.isnan(scaled_train_data)) == 0, 'NaNs in dataset'\n",
    "assert np.sum(np.isnan(scaled_val_data)) == 0, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isnan(scaled_test_data1)) == 0, 'NaNs in dataset'\n",
    "    assert np.sum(np.isnan(scaled_test_data2)) == 0, 'NaNs in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isnan(scaled_test_data)) == 0, 'NaNs in dataset'\n",
    "    \n",
    "assert np.sum(np.isinf(scaled_train_data)) == 0, 'Infinities in dataset'\n",
    "assert np.sum(np.isinf(scaled_val_data)) == 0, 'Infinities in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isinf(scaled_test_data1)) == 0, 'Infinities in dataset'\n",
    "    assert np.sum(np.isinf(scaled_test_data2)) == 0, 'Infinities in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isinf(scaled_test_data)) == 0, 'Infinities in dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c16178",
   "metadata": {},
   "source": [
    "# Get User Desired Inputs and make condensed array \n",
    "#### including x/y/z/c values, and then particle ID, event index, and number of tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.zeros((len(scaled_train_data), sample_size, len(user_input)), float)\n",
    "new_val = np.zeros((len(scaled_val_data), sample_size, len(user_input)), float)\n",
    "if TRACK_CLASS:\n",
    "    new_test1 = np.zeros((len(scaled_test_data1), sample_size, len(user_input) + 1), float)\n",
    "    new_test2 = np.zeros((len(scaled_test_data2), sample_size, len(user_input) + 1), float)\n",
    "else:\n",
    "    new_test = np.zeros((len(scaled_test_data), sample_size, len(user_input) + 1), float)\n",
    "\n",
    "\n",
    "for i,index in enumerate(user_input):\n",
    "    new_train[:,:,i] = scaled_train_data[:,:,index]\n",
    "    new_val[:,:,i] = scaled_val_data[:,:,index]\n",
    "    if TRACK_CLASS:\n",
    "        new_test1[:,:,i] = scaled_test_data1[:,:,index]\n",
    "        new_test2[:,:,i] = scaled_test_data2[:,:,index]\n",
    "    else:\n",
    "        new_test[:,:,i] = scaled_test_data[:,:,index]\n",
    "        \n",
    "if TRACK_CLASS:\n",
    "    new_test1[:,:,-1] = scaled_test_data1[:,:,6]    #saving the POINT IDs (different from particle IDs)\n",
    "    new_test2[:,:,-1] = scaled_test_data2[:,:,6]\n",
    "else:\n",
    "    new_test[:,:,-1] = scaled_test_data[:,:,6]    #saving the POINT IDs (different from particle IDs)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '_convert' + PROJECTION + '_{}'\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + '_convert' + PROJECTION + '_{}'\n",
    "\n",
    "np.save(name.format('train'), new_train)\n",
    "np.save(name.format('val'), new_val)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    np.save(name.format('test1'), new_test1)\n",
    "    np.save(name.format('test2'), new_test2)\n",
    "else:\n",
    "    np.save(name.format('test'), new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d942d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape after creating\n",
    "print(new_train.shape)\n",
    "print(new_val.shape)\n",
    "if TRACK_CLASS:\n",
    "    print(new_train[0,0,:])\n",
    "    print(new_test1.shape)\n",
    "    print(new_test2.shape)\n",
    "    \n",
    "else:\n",
    "    print(new_test.shape)\n",
    "print(new_val[0,0])\n",
    "print(new_train[0,0])\n",
    "\n",
    "assert np.sum(np.isnan(new_train)) == 0, 'NaNs in dataset'\n",
    "assert np.sum(np.isnan(new_val)) == 0, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isnan(new_test1)) == 0, 'NaNs in dataset'\n",
    "    assert np.sum(np.isnan(new_test2)) == 0, 'NaNs in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isnan(new_test)) == 0, 'NaNs in dataset'\n",
    "    \n",
    "assert np.sum(np.isinf(new_train)) == 0, 'Infinities in dataset'\n",
    "assert np.sum(np.isinf(new_val)) == 0, 'Infinities in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isinf(new_test1)) == 0, 'Infinities in dataset'\n",
    "    assert np.sum(np.isinf(new_test2)) == 0, 'Infinities in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isinf(new_test)) == 0, 'Infinities in dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95250a0d",
   "metadata": {},
   "source": [
    "# Make Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make histogram of values (either x,y,z,charge) from selected npy (train, val, test)\n",
    "# Will need to change PLOT and DATA_SET_NAME to plot X-Y-Z-Q(charge) from training, val, or test\n",
    "PLOT = 'Z'\n",
    "DATA_SET_NAME = '_train'\n",
    "index = PROJECTION.find(PLOT)\n",
    "\n",
    "if index != -1:\n",
    "    if TRACK_CLASS:\n",
    "        data = np.load('data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '_convert' + PROJECTION + DATA_SET_NAME + '.npy')\n",
    "    else:\n",
    "        data = np.load('data/' + ISOTOPE + '_size' + str(sample_size) + '_convert' + PROJECTION + DATA_SET_NAME + '.npy')\n",
    "    info = data[:,:,index].flatten()\n",
    "    plt.hist(info, density=True, bins=100)\n",
    "    plt.ylabel('probibility')\n",
    "    plt.xlabel('value')\n",
    "    plt.title(PLOT + ' Distributions')\n",
    "    plt.show()\n",
    "    # plt.savefig('data/'+ '.png', bbox_inches = 'tight') # uncomment to save\n",
    "else:\n",
    "    print('Value to plot is invalid, change PLOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd3a88-f11f-46b6-8a60-1dd8fc4e024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
